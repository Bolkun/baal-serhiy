{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys #sys.exit()\n",
    "import argparse\n",
    "from pprint import pprint\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends\n",
    "from torch import optim\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import datasets\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from baal.active import get_heuristic, ActiveLearningDataset\n",
    "from baal.active.active_loop import ActiveLearningLoop\n",
    "from baal.bayesian.dropout import patch_module\n",
    "from baal.modelwrapper import ModelWrapper\n",
    "from baal.utils.metrics import Accuracy\n",
    "from baal.active.heuristics import BALD\n",
    "\n",
    "import aug_lib\n",
    "\n",
    "from baal_extended.ExtendedActiveLearningDataset_2 import ExtendedActiveLearningDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--augment'], dest='augment', nargs=None, const=None, default=2, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epoch\", default=100, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int)  # 32\n",
    "parser.add_argument(\"--initial_pool\", default=10, type=int)   # 1000, we will start training with only 1000(org)+1000(aug)=2000 labeled data samples out of the 50k (org) and\n",
    "parser.add_argument(\"--query_size\", default=1, type=int)    # request 100(org)+100(aug)=200 new samples to be labeled at every cycle\n",
    "parser.add_argument(\"--lr\", default=0.001)\n",
    "parser.add_argument(\"--heuristic\", default=\"bald\", type=str)\n",
    "parser.add_argument(\"--iterations\", default=2, type=int)     # 20 sampling for MC-Dropout to kick paths with low weights for optimization\n",
    "parser.add_argument(\"--shuffle_prop\", default=0.05, type=float)\n",
    "parser.add_argument(\"--learning_epoch\", default=2, type=int) # 20\n",
    "parser.add_argument(\"--augment\", default=2, type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(initial_pool, n_augmentations):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(3 * [0.5], 3 * [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    aug_transform = transforms.Compose(\n",
    "        [\n",
    "            aug_lib.TrivialAugment(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(3 * [0.5], 3 * [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(3 * [0.5], 3 * [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    # Note: We use the test set here as an example. You should make your own validation set.\n",
    "    train_ds = datasets.CIFAR10(\n",
    "        \".\", train=True, transform=transform, target_transform=None, download=True\n",
    "    )\n",
    "    aug_train_ds = datasets.CIFAR10(\n",
    "        \".\", train=True, transform=aug_transform, target_transform=None, download=True\n",
    "    )\n",
    "    test_set = datasets.CIFAR10(\n",
    "        \".\", train=False, transform=test_transform, target_transform=None, download=True\n",
    "    )\n",
    "\n",
    "    #active_set = ActiveLearningDataset(train_ds, pool_specifics={\"transform\": test_transform})\n",
    "    eald_set = ExtendedActiveLearningDataset(train_ds)\n",
    "    eald_set.augment_n_times(n_augmentations, augmented_dataset=aug_train_ds)\n",
    "\n",
    "    # We start labeling randomly.\n",
    "    eald_set.label_randomly(initial_pool)\n",
    "    return eald_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pickle_file(dt_string, active_set, epoch, oracle_indices, uncertainty):    \n",
    "    pickle_filename = dt_string + (\n",
    "        f\"_uncertainty_epoch={epoch}\" f\"_labelled={len(active_set)}.pkl\"\n",
    "    )\n",
    "    dir_path = os.path.join(os.getcwd(), \"uncertainties\")\n",
    "    isExist = os.path.exists(\"uncertainties\")\n",
    "    if not isExist:\n",
    "        os.makedirs(dir_path)\n",
    "    pickle_file_path = os.path.join(dir_path, pickle_filename)\n",
    "    print(\"Saving file \" + pickle_file_path)\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"oracle_indices\": oracle_indices,\n",
    "            \"uncertainty\": uncertainty,\n",
    "            \"labelled_map\": active_set.labelled_map,\n",
    "        },\n",
    "        open(pickle_file_path, \"wb\")\n",
    "    )\n",
    "    return dir_path, pickle_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_excel_file(augment, dt_string, active_set, epoch, pickle_dir_path, df_lab_img): \n",
    "    excel_filename = dt_string + (\n",
    "        f\"_uncertainty_epoch={epoch}\" f\"_labelled={len(active_set)}.xlsx\"\n",
    "    )\n",
    "    excel_path = os.path.join(pickle_dir_path, excel_filename)\n",
    "\n",
    "    uncertainties_std = df_lab_img.transpose()\n",
    "    if augment == 1:\n",
    "        uncertainties_std.columns = ['original', 'aug1', 'std']\n",
    "    if augment == 2:   \n",
    "        uncertainties_std.columns = ['original', 'aug1', 'aug2', 'std']\n",
    "\n",
    "    uncertainties_std.to_excel(excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "orig len50000\n",
      "augmented n times0\n"
     ]
    }
   ],
   "source": [
    "args, unknown = parser.parse_known_args()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "if not use_cuda:\n",
    "    print(\"warning, the experiments would take ages to run on cpu\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%Hx%M\")\n",
    "csv_filename = \"uncertainties/metrics_cifarnet_\" + dt_string + \"_.csv\"\n",
    "with open(csv_filename, \"w+\", newline=\"\") as out_file:\n",
    "    csvwriter = csv.writer(out_file)\n",
    "    csvwriter.writerow(\n",
    "    (\n",
    "        \"epoch\",\n",
    "        \"test_acc\",\n",
    "        \"train_acc\",\n",
    "        \"test_loss\",\n",
    "        \"train_loss\",\n",
    "        \"Next training size\",\n",
    "        \"amount original images labelled\",\n",
    "        \"amount augmented images labelled\"\n",
    "    )\n",
    "    )\n",
    "\n",
    "hyperparams = vars(args)\n",
    "\n",
    "active_set, test_set = get_datasets(hyperparams[\"initial_pool\"], hyperparams[\"augment\"])\n",
    "\n",
    "heuristic = get_heuristic(hyperparams[\"heuristic\"], hyperparams[\"shuffle_prop\"])\n",
    "criterion = CrossEntropyLoss()\n",
    "model = vgg16(num_classes=10)\n",
    "\n",
    "# change dropout layer to MCDropout\n",
    "model = patch_module(model)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "else: \n",
    "    print(\"WARNING! NO CUDA IN USE!\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyperparams[\"lr\"], momentum=0.9)\n",
    "\n",
    "# Wraps the model into a usable API.\n",
    "model = ModelWrapper(model, criterion, replicate_in_memory=False)\n",
    "model.add_metric(name='accuracy', initializer=lambda : Accuracy())\n",
    "\n",
    "logs = {}\n",
    "logs[\"epoch\"] = 0\n",
    "\n",
    "# for prediction we use a smaller batchsize\n",
    "# since it is slower\n",
    "active_loop = ActiveLearningLoop(\n",
    "    active_set,\n",
    "    model.predict_on_dataset,\n",
    "    heuristic,\n",
    "    hyperparams.get(\"query_size\", 1),\n",
    "    batch_size=10,\n",
    "    iterations=hyperparams[\"iterations\"],\n",
    "    use_cuda=use_cuda,\n",
    ")\n",
    "# We will reset the weights at each active learning step.\n",
    "init_weights = deepcopy(model.state_dict())\n",
    "\n",
    "layout = {\n",
    "    \"Loss/Accuracy\": {\n",
    "        \"Loss\": [\"Multiline\", [\"loss/train\", \"loss/test\"]],\n",
    "        \"Accuracy\": [\"Multiline\", [\"accuracy/train\", \"accuracy/test\"]],\n",
    "    },\n",
    "}\n",
    "\n",
    "writer = SummaryWriter(\"vgg_mcdropout_cifar10_org+aug_3\")    # baal-serhiy/experiments/vgg_mcdropout_cifar10_org+aug_3\n",
    "writer.add_custom_scalars(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(args.epoch)):\n",
    "    # if we are in the last round we want to train for longer epochs to get a more comparable result\n",
    "    # if epoch == args.epoch:\n",
    "    #     hyperparams[\"learning_epoch\"] = 75\n",
    "    # Load the initial weights.\n",
    "    model.load_state_dict(init_weights)\n",
    "    model.train_on_dataset(\n",
    "        active_set,\n",
    "        optimizer,\n",
    "        hyperparams[\"batch_size\"],\n",
    "        hyperparams[\"learning_epoch\"],\n",
    "        use_cuda,\n",
    "    )\n",
    "\n",
    "    # Validation!\n",
    "    model.test_on_dataset(test_set, hyperparams[\"batch_size\"], use_cuda)\n",
    "    metrics = model.metrics\n",
    "\n",
    "    # get origin amount of labelled augmented/unaugmented images\n",
    "    if(epoch == 0):\n",
    "        with open(csv_filename, \"a+\", newline=\"\") as out_file:\n",
    "            csvwriter = csv.writer(out_file)\n",
    "            csvwriter.writerow(\n",
    "                (\n",
    "                -1,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                active_set.n_labelled,\n",
    "                active_set.n_unaugmented_images_labelled,\n",
    "                active_set.n_augmented_images_labelled\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # replacement for step\n",
    "    pool = active_set.pool # Returns a new Dataset made from unlabelled samples \n",
    "    print(\"1 pool length =\"+str(len(pool))) # 149970\n",
    "\n",
    "    if len(pool) > 0:\n",
    "        probs = model.predict_on_dataset(\n",
    "            pool,\n",
    "            batch_size=hyperparams[\"batch_size\"],\n",
    "            iterations=hyperparams[\"iterations\"],\n",
    "            use_cuda=use_cuda,\n",
    "        )\n",
    "\n",
    "        #if probs is not None and (isinstance(probs, types.GeneratorType) or len(probs) > 0):\n",
    "        # -> \"isinstance(...) needed when using predict_..._Generator\"\n",
    "        if probs is not None and len(probs) > 0:\n",
    "            # 1. Get uncertainty\n",
    "            uncertainty = active_loop.heuristic.get_uncertainties(probs)\n",
    "            oracle_indices = np.argsort(uncertainty)\n",
    "            active_set.labelled_map\n",
    "\n",
    "            print(\"2 oracle_indices length \"+str(len(oracle_indices))) # 149970\n",
    "\n",
    "            pickle_dir_path, pickle_file_path = generate_pickle_file(dt_string, active_set, epoch, oracle_indices, uncertainty)\n",
    "\n",
    "            mypickle = pd.read_pickle(pickle_file_path)\n",
    "\n",
    "            uncertainty = mypickle['uncertainty']\n",
    "            oracle_indices = mypickle['oracle_indices']\n",
    "            labelled_map = mypickle['labelled_map']\n",
    "\n",
    "            orig_s2 = int((len(pool)/3)-1)\n",
    "            aug1_s1 = int(len(pool)/3)\n",
    "            aug1_s2 = int((len(pool)/3)*2-1)\n",
    "            aug2_s1 = int((len(pool)/3)*2)\n",
    "            aug2_s2 = int(len(pool)-1)\n",
    "\n",
    "            original = uncertainty[0:orig_s2]\n",
    "            aug1 = uncertainty[aug1_s1:aug1_s2]\n",
    "            aug2 = uncertainty[aug2_s1:aug2_s2]\n",
    "\n",
    "            print(\"3 original length \"+str(len(original)))\n",
    "            print(\"4 aug1 length \"+str(len(aug1)))\n",
    "            print(\"5 aug2 length \"+str(len(aug2)))\n",
    "\n",
    "            if hyperparams[\"augment\"] == 1:\n",
    "                matrix = np.vstack([original, aug1])\n",
    "            if hyperparams[\"augment\"] == 2:   \n",
    "                matrix = np.vstack([original, aug1, aug2])\n",
    "\n",
    "            # 2. Calc standard deviation\n",
    "            df_lab_img = pd.DataFrame(matrix)\n",
    "            df_lab_img.std() # here\n",
    "            df_lab_img = pd.DataFrame(np.vstack([matrix, df_lab_img.std()]))\n",
    "\n",
    "            uncertainties_std = df_lab_img.transpose()\n",
    "            if hyperparams[\"augment\"] == 1:\n",
    "                uncertainties_std.columns = ['original', 'aug1', 'std']\n",
    "            if hyperparams[\"augment\"] == 2:   \n",
    "                uncertainties_std.columns = ['original', 'aug1', 'aug2', 'std']\n",
    "\n",
    "            generate_excel_file(hyperparams[\"augment\"], dt_string, active_set, epoch, pickle_dir_path, df_lab_img)\n",
    "            \n",
    "            # 3. Map std uncertainties to uncertainty array\n",
    "            std_array = df_lab_img.std()\n",
    "            for i in range(len(uncertainty)): # 150000\n",
    "                uncertainty[i] = std_array[i % (len(pool)/3-1)]\n",
    "            oracle_indices = np.argsort(uncertainty)\n",
    "            print(\"6 oracle_indices length \"+str(len(oracle_indices)))\n",
    "            active_set.labelled_map\n",
    "            # to_label -> indices sortiert von größter zu niedrigster uncertainty\n",
    "            # uncertainty -> alle std uncertainties des pools\n",
    "            to_label = heuristic.reorder_indices(uncertainty)\n",
    "            print(\"7 to_label length \"+str(len(to_label)))\n",
    "            to_label = oracle_indices[np.array(to_label)] # len(to_label) = 150000\n",
    "            print(\"8 to_label length \"+str(len(to_label)))\n",
    "            if len(to_label) > 0:\n",
    "                active_set.label(to_label[: hyperparams.get(\"query_size\", 1)])\n",
    "            else: break\n",
    "        else:\n",
    "            break\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 8]\n",
      "[6, 9, 2]\n",
      "[1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "original = [3, 7, 8]\n",
    "aug1 = [6, 9, 2]\n",
    "aug2 = [1, 1, 1]\n",
    "\n",
    "if len(original) != len(aug1) or len(original) != len(aug2) or len(aug1) != len(aug2):\n",
    "  if len(original) > len(aug1):\n",
    "    aug1 += (len(original)-len(aug1)) * [0]\n",
    "  else:\n",
    "    original += (len(aug1)-len(original)) * [0]\n",
    "  if len(original) > len(aug2):\n",
    "    aug2 += (len(original)-len(aug2)) * [0]\n",
    "  else:\n",
    "    original += (len(aug2)-len(original)) * [0]\n",
    "  if len(aug1) > len(aug2):\n",
    "    aug2 += (len(aug1)-len(aug2)) * [0]\n",
    "  else:\n",
    "    aug1 += (len(aug2)-len(aug1)) * [0]\n",
    "\n",
    "\n",
    "print (original)\n",
    "print (aug1)\n",
    "print (aug2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deepAugmentEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcb64cc0b8f44bc3cb82af8cffebd80c81d2744dc28859d8b272056cb1c5b84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
